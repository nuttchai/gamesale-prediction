{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.svm import SVR\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./Video_Games_Sales_as_at_22_Dec_2016.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We was thinking that critic score will be one of the main features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    -Droping games without a year of release or genre\n",
    "    -Renaming columns for ease of use\n",
    "    -Creating a new column for age of the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename(columns={\"Year_of_Release\": \"Year\", \n",
    "                            \"NA_Sales\": \"NA\",\n",
    "                            \"EU_Sales\": \"EU\",\n",
    "                            \"JP_Sales\": \"JP\",\n",
    "                            \"Other_Sales\": \"Other\",\n",
    "                            \"Global_Sales\": \"Global\"})\n",
    "data = data[data[\"Year\"].notnull()]\n",
    "data = data[data[\"Genre\"].notnull()]\n",
    "data[\"Year\"] = data[\"Year\"].apply(int)\n",
    "data[\"Age\"] = 2021 - data[\"Year\"]\n",
    "data.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram plot of Year of release\n",
    "num_years = data[\"Year\"].max() - data[\"Year\"].min() + 1\n",
    "plt.hist(data[\"Year\"], bins=num_years, color=\"lightskyblue\", edgecolor=\"black\")\n",
    "plt.title(\"Distribution of year of release\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Number of games\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing \"tbd\" values with np.nan and transforming column to float type\n",
    "data[\"User_Score\"] = data[\"User_Score\"].replace(\"tbd\", np.nan).astype(float)\n",
    "\n",
    "g = sns.jointplot(x=\"User_Score\", y=\"Critic_Score\", data=data, cmap=\"Blues\", kind=\"hex\", \n",
    "                  height=10);\n",
    "g.ax_marg_x.grid(False)\n",
    "g.ax_marg_y.grid(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next We want to see how many values there are missing in each column. We are using a function We found in one of [William Koehrsen](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering) blogs on Medium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate missing values by column\n",
    "def missing_values_table(df):\n",
    "        # Total missing values\n",
    "        mis_val = df.isnull().sum()\n",
    "        \n",
    "        # Percentage of missing values\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        \n",
    "        # Make a table with the results\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        \n",
    "        # Rename the columns\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : \"Missing Values\", 1 : \"% of Total Values\"})\n",
    "        \n",
    "        # Sort the table by percentage of missing descending\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        \"% of Total Values\", ascending=False).round(1)\n",
    "        \n",
    "        # Print some summary information\n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        \n",
    "        # Return the dataframe with missing information\n",
    "        return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_table(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_outliers(df, list_of_keys):\n",
    "    df_out = df\n",
    "    for key in list_of_keys:\n",
    "        # Calculate first and third quartile\n",
    "        first_quartile = df_out[key].describe()[\"25%\"]\n",
    "        third_quartile = df_out[key].describe()[\"75%\"]\n",
    "\n",
    "        # Interquartile range\n",
    "        iqr = third_quartile - first_quartile\n",
    "\n",
    "        # Remove outliers\n",
    "        removed = df_out[(df_out[key] <= (first_quartile - 3 * iqr)) |\n",
    "                    (df_out[key] >= (third_quartile + 3 * iqr))] \n",
    "        df_out = df_out[(df_out[key] > (first_quartile - 3 * iqr)) &\n",
    "                    (df_out[key] < (third_quartile + 3 * iqr))]\n",
    "    return df_out, removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, rmvd_global = rm_outliers(data, [\"Global\"])\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Has_Score\"] = data[\"User_Score\"].notnull() & data[\"Critic_Score\"].notnull()\n",
    "rmvd_global[\"Has_Score\"] = rmvd_global[\"User_Score\"].notnull() & rmvd_global[\"Critic_Score\"].notnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "plt.hist(data[data[\"Has_Score\"]==True][\"Global\"], color=\"limegreen\", alpha=0.5, \n",
    "         edgecolor=\"black\")\n",
    "plt.hist(data[data[\"Has_Score\"]==False][\"Global\"], color=\"indianred\", alpha=0.5, \n",
    "         edgecolor=\"black\")\n",
    "plt.title(\"Distribution of global sales\")\n",
    "plt.xlabel(\"Global sales, $M\")\n",
    "plt.ylabel(\"Number of games\")\n",
    "plt.legend(handles=[Line2D([0], [0], color=\"limegreen\", lw=20, label=\"True\", alpha=0.5),\n",
    "                    Line2D([0], [0], color=\"indianred\", lw=20, label=\"False\", alpha=0.5)],\n",
    "           title=\"Has_Score\", loc=7);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model_v2 :\n",
    "    - drop game without score (critic or user)\n",
    "    - use scores to predict sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored = data.dropna(subset=[\"User_Score\", \"Critic_Score\", \"Rating\"])\n",
    "scored.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove outliers in User_Count column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored, rmvd_user_count = rm_outliers(scored, [\"User_Count\"])\n",
    "scored.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numeric Column - age\n",
    "\n",
    "one-hot encoded categorical columns - platform, genre, rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "# Select the numeric columns\n",
    "numeric_subset = scored.select_dtypes(\"number\").drop(columns=[\"NA\", \"EU\", \"JP\", \"Other\", \"Year\"])\n",
    "\n",
    "# Select the categorical column\n",
    "categorical_subset = scored[[\"Platform\", \"Genre\", \"Rating\"]]\n",
    "\n",
    "# One hot encode\n",
    "encoder = ce.one_hot.OneHotEncoder()\n",
    "categorical_subset = encoder.fit_transform(categorical_subset)\n",
    "\n",
    "# Join the two dataframes using concat\n",
    "# Make sure to use axis = 1 to perform a column bind\n",
    "features = pd.concat([numeric_subset, categorical_subset], axis = 1)\n",
    "\n",
    "# Find correlations with the score \n",
    "correlations = features.corr()[\"Global\"].dropna().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the columns to plot\n",
    "plot_data = features[[\"Global\", \"Critic_Score\", \"User_Score\",\n",
    "                      \"Critic_Count\", \"User_Count\"]]\n",
    "\n",
    "# Function to calculate correlation coefficient between two columns\n",
    "def corr_func(x, y, **kwargs):\n",
    "    r = np.corrcoef(x, y)[0][1]\n",
    "    ax = plt.gca()\n",
    "    ax.annotate(\"r = {:.2f}\".format(r),\n",
    "                xy=(.2, .8), xycoords=ax.transAxes,\n",
    "                size = 20)\n",
    "    \n",
    "# Create the pairgrid object\n",
    "grid = sns.PairGrid(data = plot_data, size = 3)\n",
    "\n",
    "# Upper is a scatter plot\n",
    "grid.map_upper(plt.scatter, color = \"lightskyblue\", alpha = 0.6, marker=\".\", s=10)\n",
    "\n",
    "# Diagonal is a histogram\n",
    "grid.map_diag(plt.hist, color = \"lightskyblue\", edgecolor=\"black\")\n",
    "\n",
    "# Bottom is correlation and density plot\n",
    "grid.map_lower(corr_func)\n",
    "grid.map_lower(sns.kdeplot, cmap = plt.cm.Blues)\n",
    "\n",
    "# Title for entire plot\n",
    "plt.suptitle(\"Pairs Plot of Game Scores\", size = 36, y = 1.02);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 39 features (1 is target) in the dataset after feature engineering and selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting data into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "basic_target = pd.Series(features[\"Global\"])\n",
    "basic_features = features.drop(columns=\"Global\")\n",
    "features_train, features_test, target_train, target_test = train_test_split(basic_features, basic_target, \n",
    "                                                                            test_size=0.2,\n",
    "                                                                            random_state=42)\n",
    "print(features_train.shape)\n",
    "print(features_test.shape)\n",
    "print(target_train.shape)\n",
    "print(target_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(y_true, y_pred):\n",
    "    return np.average(abs(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_guess = np.median(target_train)\n",
    "basic_baseline_mae = mae(target_test, baseline_guess)\n",
    "print(baseline_guess)\n",
    "print(basic_baseline_mae)\n",
    "print(\"Baseline guess for global sales is: {:.02f}\".format(baseline_guess))\n",
    "print(\"Baseline Performance on the test set: MAE = {:.04f}\".format(basic_baseline_mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare several simple models with different types of regression, and then focus on the best one for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_evaluate(model):\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(features_train, target_train)\n",
    "    \n",
    "    # Make predictions and evalute\n",
    "    model_pred = model.predict(features_test)\n",
    "    model_mae = mae(target_test, model_pred)\n",
    "    print(model_pred)\n",
    "    # Return the performance metric\n",
    "#     return model_mae\n",
    "    return model_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_mae = fit_and_evaluate(lr)\n",
    "#print(np.mean(lr_mae))\n",
    "print(\"Linear Regression Performance on the test set: MAE = {:.04f}\".format(np.mean(lr_mae)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### what should be a good value for K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_maelst = []\n",
    "for i in range(1,50):\n",
    "    knn = KNeighborsRegressor(n_neighbors=i)\n",
    "    knn_mae = fit_and_evaluate(knn)\n",
    "    knn_maelst.append(knn_mae)\n",
    "print(\"K-Nearest Neighbors Regression Performance on the test set: MAE = {:.04f}\".format(knn_mae))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knntest = KNeighborsRegressor(n_neighbors=3)\n",
    "knn_mae = fit_and_evaluate(knntest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1,50),knn_maelst,color='blue', linestyle='dashed', marker='o',\n",
    "         markerfacecolor='red', markersize=10)\n",
    "plt.title('Error Rate vs. K Value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Error Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find index of lowest knn mae "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(knn_maelst)\n",
    "print(np.where(a == a.min()))\n",
    "knn_mae = a.min()\n",
    "a.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add more in order to have graph like the example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVR(C = 1000, gamma=0.1)\n",
    "svm_mae = fit_and_evaluate(svm)\n",
    "\n",
    "print(\"Support Vector Machine Regression Performance on the test set: MAE = {:.04f}\".format(svm_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "random_forest = RandomForestRegressor(random_state=60)\n",
    "random_forest_mae = fit_and_evaluate(random_forest)\n",
    "\n",
    "print(\"Random Forest Regression Performance on the test set: MAE = {:.04f}\".format(random_forest_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_boosting = GradientBoostingRegressor(random_state=60)\n",
    "gradient_boosting_mae = fit_and_evaluate(gradient_boosting)\n",
    "\n",
    "print(\"Gradient Boosting Regression Performance on the test set: MAE = {:.04f}\".format(gradient_boosting_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha=10)\n",
    "ridge_mae = fit_and_evaluate(ridge)\n",
    "\n",
    "print(\"Ridge Regression Performance on the test set: MAE = {:.04f}\".format(ridge_mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Mode coparison ( LR vs KNN vs Baseline )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comparison = pd.DataFrame({\"model\": [\"Linear Regression\", \"Support Vector Machine\",\n",
    "                                           \"Random Forest\", \"Gradient Boosting\",\n",
    "                                            \"K-Nearest Neighbors\", \"Baseline (median)\", \"Ridge\"],\n",
    "                                 \"mae\": [lr_mae, svm_mae, random_forest_mae, \n",
    "                                         gradient_boosting_mae, knn_mae, basic_baseline_mae, ridge_mae]})\n",
    "model_comparison.sort_values(\"mae\", ascending=False).plot(x=\"model\", y=\"mae\", kind=\"barh\",\n",
    "                                                           color=\"lightskyblue\", legend=False)\n",
    "plt.ylabel(\"\"); plt.yticks(size=14); plt.xlabel(\"Mean Absolute Error\"); plt.xticks(size=14)\n",
    "plt.title(\"Model Comparison on Test MAE\", size=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Bossting seems to be the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue by hrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function to be optimized\n",
    "loss = [\"ls\", \"lad\", \"huber\"]\n",
    "\n",
    "# Maximum depth of each tree\n",
    "max_depth = [2, 3, 5, 10, 15]\n",
    "\n",
    "# Minimum number of samples per leaf\n",
    "min_samples_leaf = [1, 2, 4, 6, 8]\n",
    "\n",
    "# Minimum number of samples to split a node\n",
    "min_samples_split = [2, 4, 6, 10]\n",
    "\n",
    "# Maximum number of features to consider for making splits\n",
    "max_features = [\"auto\", \"sqrt\", \"log2\", None]\n",
    "\n",
    "hyperparameter_grid = {\"loss\": loss,\n",
    "                       \"max_depth\": max_depth,\n",
    "                       \"min_samples_leaf\": min_samples_leaf,\n",
    "                       \"min_samples_split\": min_samples_split,\n",
    "                       \"max_features\": max_features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "basic_model = GradientBoostingRegressor(random_state = 42)\n",
    "\n",
    "random_cv = RandomizedSearchCV(estimator=basic_model,\n",
    "                               param_distributions=hyperparameter_grid,\n",
    "                               cv=4, n_iter=20, \n",
    "                               scoring=\"neg_mean_absolute_error\",\n",
    "                               n_jobs=-1, verbose=1, \n",
    "                               return_train_score=True)\n",
    "                   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_cv.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are printing out 10 best estimators found by randomized search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_results = pd.DataFrame(random_cv.cv_results_).sort_values(\"mean_test_score\", ascending=False)\n",
    "random_results.head(10)[[\"mean_test_score\", \"param_loss\",\n",
    "                         \"param_max_depth\", \"param_min_samples_leaf\", \"param_min_samples_split\",\n",
    "                         \"param_max_features\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "trees_grid = {\"n_estimators\": [50, 100, 150, 200, 250, 300]}\n",
    "\n",
    "basic_model = random_cv.best_estimator_\n",
    "grid_search = GridSearchCV(estimator=basic_model, param_grid=trees_grid, cv=4, \n",
    "                           scoring=\"neg_mean_absolute_error\", verbose=1,\n",
    "                           n_jobs=-1, return_train_score=True)\n",
    "grid_search.fit(features_train, target_train);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to see that did the numbers of trees is going to effects the performance or not in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "plt.plot(results[\"param_n_estimators\"], -1 * results[\"mean_test_score\"], label = \"Testing Error\")\n",
    "plt.plot(results[\"param_n_estimators\"], -1 * results[\"mean_train_score\"], label = \"Training Error\")\n",
    "plt.xlabel(\"Number of Trees\"); plt.ylabel(\"Mean Abosolute Error\"); plt.legend();\n",
    "plt.title(\"Performance vs Number of Trees\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph shows that the model is overfitting. Training error keeps decreasing, while test error stays almost the same. It means that the model learns training examples very well, but cannot generalize on new, unknown data. This is not a very good model, but I will leave it as is, and try to battle overfitting in the advanced model using imputing, feature selection and feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_final_model = grid_search.best_estimator_\n",
    "basic_final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "basic_final_pred = basic_final_model.predict(features_test)\n",
    "basic_final_mae = mae(target_test, basic_final_pred)\n",
    "print(\"Final model performance on the test set: MAE = {:.04f}.\".format(basic_final_mae))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAE dropped, but by a very small margin. Looks like hyperparameter tuning didn't really improve the model. I hope advanced model will have a better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "To finish with the basic model I am going to draw 2 graphs. First one is comparison of densities of train values, test values and predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(basic_final_pred, label = \"Predictions\")\n",
    "sns.kdeplot(target_test, label = \"Test\")\n",
    "sns.kdeplot(target_train, label = \"Train\")\n",
    "\n",
    "plt.xlabel(\"Global Sales\"); \n",
    "plt.ylabel(\"Density\");\n",
    "plt.title(\"Test, Train Values and Predictions\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Predictions density is moved a little to the right, comparing to densities of initial values. The tail is also different. This might help tuning the model in the future.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Second graph is a histogram of residuals - differences between real values and predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "basic_residuals = basic_final_pred - target_test\n",
    "\n",
    "sns.kdeplot(basic_residuals, color = \"lightskyblue\")\n",
    "plt.xlabel(\"Error\"); plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Residuals\");\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def donut_chart(column, palette=\"Set2\"):\n",
    "    values = column.value_counts().values\n",
    "    labels = column.value_counts().index\n",
    "    plt.pie(values, colors=sns.color_palette(palette), \n",
    "            labels=labels, autopct=\"%1.1f%%\", \n",
    "            startangle=90, pctdistance=0.85)\n",
    "    #draw circle\n",
    "    centre_circle = plt.Circle((0,0), 0.70, fc=\"white\")\n",
    "    fig = plt.gcf()\n",
    "    fig.gca().add_artist(centre_circle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donut_chart(data[\"Platform\"])\n",
    "plt.title(\"Platforms\")\n",
    "plt.axis(\"equal\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Platform\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "too many platform so we are going to group them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "platforms = {\"Playstation\" : [\"PS\", \"PS2\", \"PS3\", \"PS4\"],\n",
    "             \"Xbox\" : [\"XB\", \"X360\", \"XOne\"], \n",
    "             \"PC\" : [\"PC\"],\n",
    "             \"Nintendo\" : [\"Wii\", \"WiiU\"],\n",
    "             \"Portable\" : [\"GB\", \"GBA\", \"GC\", \"DS\", \"3DS\", \"PSP\", \"PSV\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_label(x, groups=None):\n",
    "    if groups is None:\n",
    "        return \"Other\"\n",
    "    else:\n",
    "        for key, val in groups.items():\n",
    "            if x in val:\n",
    "                return key\n",
    "        return \"Other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Grouped_Platform\"] = data[\"Platform\"].apply(lambda x: get_group_label(x, groups=platforms))\n",
    "donut_chart(data[\"Grouped_Platform\"])\n",
    "plt.title(\"Groups of platforms\")\n",
    "plt.axis(\"equal\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donut_chart(data[\"Genre\"], palette=\"muted\")\n",
    "plt.title(\"Genres\")\n",
    "plt.axis(\"equal\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored[\"Grouped_Platform\"] = scored[\"Platform\"].apply(lambda x: get_group_label(x, platforms))\n",
    "donut_chart(scored[\"Grouped_Platform\"])\n",
    "plt.title(\"Groups of platforms for games with score\")\n",
    "plt.axis(\"equal\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "scored[scored[\"Grouped_Platform\"]==\"Other\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
